{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  6.2832,  12.5664,  25.1327,  50.2655, 100.5310],\n",
      "        [ 12.5664,  25.1327,  50.2655, 100.5310, 201.0619]])\n",
      "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.7485e-07,\n",
      "         3.4969e-07, 6.9938e-07, 1.3988e-06, 2.7975e-06],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 3.4969e-07,\n",
      "         6.9938e-07, 1.3988e-06, 2.7975e-06, 5.5951e-06]])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "def forward(x: th.Tensor, levels: int, scale: float) -> th.Tensor:\n",
    "    \"\"\"\n",
    "    Gets the positional encoding of x for each channel.\n",
    "    x_i in [-0.5, 0.5] -> function(x_i * pi * 2^j) for function in (cos, sin) for j in [0, levels-1]\n",
    "    \"\"\"\n",
    "    scale = scale*(2**th.arange(levels, device=x.device)).repeat(x.shape[1])\n",
    "    args = x.repeat_interleave(levels, dim=1) * scale\n",
    "    print(args)\n",
    "\n",
    "    return th.hstack((th.cos(args), th.sin(args)))\n",
    "\n",
    "test_tensor = th.Tensor([[0], [1], [2]])\n",
    "levels = 5 \n",
    "scale = 2*th.pi\n",
    "\n",
    "print(forward(test_tensor, levels, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of \n",
    "# x y z repeat levels times then repeat once more for cos and sin\n",
    "\n",
    "def calculate_mask(alpha, k):\n",
    "    if alpha < k: \n",
    "        return 0 \n",
    "    if 0 <= k - alpha < 1: \n",
    "        return (1 - cos((alpha - k)/ pi)) /2\n",
    "    else: \n",
    "        return 1 \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3796,  0.3098, -1.2437,  0.0544,  0.1402, -1.5004,  0.3479, -0.6451],\n",
      "        [-0.3724,  0.5560,  2.0618,  0.3816,  2.9674, -1.0613,  0.5431, -0.2598],\n",
      "        [-1.1162, -1.1075, -0.4582, -0.5607,  0.6536, -0.0932,  1.0657, -0.5071],\n",
      "        [-0.1248, -2.2781,  1.4051, -0.9471, -1.8571,  0.3086, -0.0688, -1.0713],\n",
      "        [-0.8725, -1.4538, -0.5265,  0.4309, -1.1718,  0.6643,  1.3452, -0.5451]])\n",
      "tensor([[ 0.1898,  0.0000, -0.0000,  0.0000,  0.0701, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.1862,  0.0000,  0.0000,  0.0000,  1.4837, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.5581, -0.0000, -0.0000, -0.0000,  0.3268, -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0624, -0.0000,  0.0000, -0.0000, -0.9286,  0.0000, -0.0000, -0.0000],\n",
      "        [-0.4363, -0.0000, -0.0000,  0.0000, -0.5859,  0.0000,  0.0000, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _get_mask(alpha, levels: int) -> th.Tensor:\n",
    "        \"\"\"\n",
    "        Get the mask from the barf paper that fits the positional encodings\n",
    "        \"\"\"\n",
    "        def get_mask(alpha: th.Tensor, k: th.Tensor) -> th.Tensor:\n",
    "            \"\"\"\n",
    "            Calculates the mask from the barf paper given alpha and k. \n",
    "\n",
    "            Args:\n",
    "                alpha (float): value that is proporitional to the batch\n",
    "                k (float):     the level of the positional encoding\n",
    "            \n",
    "            Returns:\n",
    "                float: 0 if alpha < k, 1 if alpha - k >= 1 and a cosine interpolation otherwise\n",
    "            \"\"\"\n",
    "            result = th.zeros_like(alpha)\n",
    "            \n",
    "            condition1 = alpha - k < 0\n",
    "            condition2 = (0 <= alpha - k) & (alpha - k < 1)\n",
    "            \n",
    "            result[condition1] = 0\n",
    "            result[condition2] = (1 - th.cos((alpha[condition2] - k[condition2]) * th.pi)) / 2\n",
    "            result[~(condition1 | condition2)] = 1\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # Create a vector of alpha values\n",
    "        # alpha = 1 # TODO fix this by getting the current epoch \n",
    "        alpha = th.ones((levels)) * alpha\n",
    "        k = th.arange(levels)\n",
    "\n",
    "        # Get the mask vector \n",
    "        mask = get_mask(alpha, k)\n",
    "\n",
    "        # Reshape mask to take in (x,y,z) and repeat an extra time for sin/cos \n",
    "        mask = mask.repeat_interleave(1)\n",
    "        mask = mask.repeat(2) \n",
    "\n",
    "        return mask\n",
    "\n",
    "mask = _get_mask(0.5, 4).unsqueeze(0)\n",
    "\n",
    "test_tensor = th.randn(5, 8)\n",
    "print(test_tensor)\n",
    "\n",
    "print(test_tensor * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "levels = 5 \n",
    "\n",
    "# Create a vector of alpha values\n",
    "alpha = 1 # TODO fix this by getting the current epoch \n",
    "alpha = th.ones((levels)) * alpha\n",
    "k = th.arange(levels)\n",
    "\n",
    "result = th.zeros_like(alpha)\n",
    "\n",
    "condition1 = alpha < k\n",
    "condition2 = (0 <= k - alpha) & (k - alpha < 1)\n",
    "\n",
    "result[condition1] = 0\n",
    "result[condition2] = (1 - th.cos((alpha[condition2] - k[condition2]) / th.pi)) / 2\n",
    "result[~(condition1 | condition2)] = 1\n",
    "\n",
    "# Reshape mask to take in (x,y,z) and repeat an extra time for sin/cos \n",
    "mask = result.repeat_interleave(3)\n",
    "mask = mask.tile(2) \n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataloader(self, images: list[int], dataset: ImagePoseDataset)-> DataLoader:\n",
    "        \"\"\"Get a subset of a dataloader with the specified images.\"\"\"\n",
    "        # Slice the dataset\n",
    "        \n",
    "        # Return subset of dataset\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.dataloader_kwargs[\"batch_size\"],\n",
    "            num_workers=self.dataloader_kwargs[\"num_workers\"],\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            sampler=th.utils.data.SubsetRandomSampler(indices)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import math\n",
    "import sys\n",
    "from typing import Callable, Optional, cast\n",
    "\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision as tv\n",
    "\n",
    "\n",
    "# Type alias for dataset output\n",
    "#  (origin_raw, origin_noisy, direction_raw, direction_noisy, pixel_color_raw, pixel_color_blur, pixel_relative_blur, image_index)\n",
    "DatasetOutput = tuple[th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor]\n",
    "\n",
    "\n",
    "class ImagePoseDataset(Dataset[DatasetOutput]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_width: int,\n",
    "        image_height: int,\n",
    "        images_path: str,\n",
    "        camera_info_path: str,\n",
    "        space_transform_scale: Optional[float]=None,\n",
    "        space_transform_translate: Optional[th.Tensor]=None,\n",
    "        rotation_noise_sigma: float=1.0,\n",
    "        translation_noise_sigma: float=1.0,\n",
    "        noise_seed: Optional[int]=None,\n",
    "        gaussian_blur_kernel_size: int=40,\n",
    "        gaussian_blur_relative_sigma_start: float=0.,\n",
    "        gaussian_blur_relative_sigma_decay: float=1.\n",
    "    ) -> None:\n",
    "        \"\"\"Loads images, camera info, and generates rays for each pixel in each image.\n",
    "        \n",
    "        Args:\n",
    "            image_width (int): Width to resize images to.\n",
    "            image_height (int): Height to resize images to.\n",
    "            images_path (str): Path to the image directory.\n",
    "            camera_info_path (str): Path to the camera info file.\n",
    "            space_transform_scale (Optional[float], optional): Scale parameter for the space transform. Defaults to None, which auto-calculates based on max distance.\n",
    "            space_transform_translate (Optional[th.Tensor], optional): Translation parameter for the space transform. Defaults to None, which auto-calculates the mean.\n",
    "            rotation_noise_sigma (float, optional): Sigma parameter for the rotation noise in radians. Defaults to 1.0.\n",
    "            translation_noise_sigma (float, optional): Sigma parameter for the translation noise. Defaults to 1.0.\n",
    "            noise_seed (Optional[int], optional): Seed for the noise generator. Defaults to None.\n",
    "            gaussian_blur_kernel_size (int, optional): Size of the gaussian blur kernel. Defaults to 5.\n",
    "            gaussian_blur_relative_sigma_start (float, optional): Initial sigma parameter for the gaussian blur. Set to 0 to disable. Defaults to 0..\n",
    "            gaussian_blur_relative_sigma_decay (float, optional): Decay factor for the gaussian blur sigma. Defaults to 1..\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Verify parameters\n",
    "        if gaussian_blur_kernel_size % 2 == 0:\n",
    "            raise ValueError(\"Gaussian blur kernel size must be odd.\")\n",
    "\n",
    "\n",
    "        # Store image dimensions\n",
    "        self.image_height, self.image_width = image_width, image_height\n",
    "        self.image_batch_size = self.image_width * self.image_height\n",
    "\n",
    "        # Store paths\n",
    "        self.images_path = images_path\n",
    "        self.camera_info_path = camera_info_path\n",
    "\n",
    "        # Store noise parameters\n",
    "        # NOTE: Rotation is measured in radians\n",
    "        self.rotation_noise_sigma = rotation_noise_sigma\n",
    "        self.translation_noise_sigma = translation_noise_sigma\n",
    "        self.noise_seed = noise_seed\n",
    "\n",
    "        # Store gaussian blur parameters\n",
    "        self.gaussian_blur_kernel_size = gaussian_blur_kernel_size\n",
    "        self.gaussian_blur_relative_sigma_start = gaussian_blur_relative_sigma_start\n",
    "        self.gaussian_blur_relative_sigma_decay = gaussian_blur_relative_sigma_decay\n",
    "        self.gaussian_blur_relative_sigma_current = self.gaussian_blur_relative_sigma_start\n",
    "\n",
    "\n",
    "\n",
    "        # Load images\n",
    "        self.images = self._load_images(\n",
    "            self.images_path, \n",
    "            self.image_width, \n",
    "            self.image_height\n",
    "        )\n",
    "\n",
    "        # Load camera info\n",
    "        self.focal_length, self.camera_to_world = self._load_camera_info(\n",
    "            self.camera_info_path, \n",
    "            self.image_width\n",
    "        )\n",
    "\n",
    "        # Transform camera to world matrices\n",
    "        (\n",
    "            self.camera_to_world, \n",
    "            self.space_transform_scale, \n",
    "            self.space_transform_translate\n",
    "        ) = self._transform_camera_to_world(\n",
    "            self.camera_to_world, \n",
    "            space_transform_scale,\n",
    "            space_transform_translate\n",
    "        )\n",
    "\n",
    "\n",
    "        # Get gaussian blur kernel\n",
    "        self.gaussian_blur_kernel = self._get_gaussian_blur_kernel(\n",
    "            self.gaussian_blur_kernel_size,\n",
    "            self.gaussian_blur_relative_sigma_current,\n",
    "            max(self.image_height, self.image_width)\n",
    "        )\n",
    "\n",
    "\n",
    "        # Get raw rays for each pixel in each image\n",
    "        self.origins_raw, self.directions_raw = self._get_raw_rays(\n",
    "            self.camera_to_world, \n",
    "            self.image_width, \n",
    "            self.image_height, \n",
    "            self.focal_length\n",
    "        )\n",
    "        \n",
    "        # Get (artificially) noisy rays for each pixel in each image\n",
    "        self.origins_noisy, self.directions_noisy = self._get_noisy_rays(\n",
    "            self.origins_raw,\n",
    "            self.directions_raw,\n",
    "            self.rotation_noise_sigma,\n",
    "            self.translation_noise_sigma,\n",
    "            self.noise_seed\n",
    "        )\n",
    "\n",
    "\n",
    "        # Store dataset output\n",
    "        self.dataset = [\n",
    "            (\n",
    "                camera_to_world, \n",
    "                self.origins_raw[image_name],\n",
    "                self.origins_noisy[image_name],\n",
    "                self.directions_raw[image_name], \n",
    "                self.directions_noisy[image_name],\n",
    "                self.images[image_name]\n",
    "            ) \n",
    "            for image_name, camera_to_world in self.camera_to_world.items()\n",
    "        ]\n",
    "\n",
    "    def _load_images(self, image_dir_path, image_width, image_height) -> dict[str, th.Tensor]:\n",
    "        # Transform image to correct format\n",
    "        transform = cast(\n",
    "            Callable[[th.Tensor], th.Tensor], \n",
    "            tv.transforms.Compose([\n",
    "                # Convert to float\n",
    "                tv.transforms.Lambda(lambda img: img.float() / 255.),\n",
    "                # Resize image\n",
    "                tv.transforms.Resize(\n",
    "                    (image_height, image_width), \n",
    "                    interpolation=tv.transforms.InterpolationMode.BICUBIC, \n",
    "                    antialias=True # type: ignore\n",
    "                ),\n",
    "                # Transform alpha to white background (removes alpha too)\n",
    "                tv.transforms.Lambda(lambda img: img[-1] * img[:3] + (1 - img[-1])),\n",
    "                # Permute channels to (H, W, C)\n",
    "                # WARN: This is against the convention of PyTorch.\n",
    "                #  Doing it to enable easier batching of rays.\n",
    "                tv.transforms.Lambda(lambda img: img.permute(1, 2, 0))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        # Open RGBA image\n",
    "        read = lambda path: tv.io.read_image(\n",
    "            os.path.join(image_dir_path, path), \n",
    "            tv.io.ImageReadMode.RGB_ALPHA\n",
    "        )\n",
    "\n",
    "        # Load each image, transform, and store\n",
    "        return {\n",
    "            pathlib.PurePath(path).stem: transform(read(path))\n",
    "            for path in os.listdir(image_dir_path) \n",
    "        }\n",
    "\n",
    "    def _load_camera_info(self, camera_info_path: str, image_width: int) -> tuple[float, dict[str, th.Tensor]]:\n",
    "        # Read info file\n",
    "        camera_data = json.loads(open(camera_info_path).read())\n",
    "        \n",
    "        # Calculate focal length from camera horizontal angle\n",
    "        focal_length = image_width / 2 / math.tan(camera_data[\"camera_angle_x\"] / 2)\n",
    "        # Get camera to world matrices\n",
    "        # NOTE: Projections are scaled to have scale 1\n",
    "        camera_to_world: dict[str, th.Tensor] = { \n",
    "            pathlib.PurePath(path).stem: th.tensor(camera_to_world) / camera_to_world[-1][-1]\n",
    "            for frame in camera_data[\"frames\"] \n",
    "            for path, rotation, camera_to_world in [frame.values()] \n",
    "        }\n",
    "\n",
    "        # Return focal length and camera to world matrices\n",
    "        return focal_length, camera_to_world\n",
    "\n",
    "    def _transform_camera_to_world(self, camera_to_world: dict[str, th.Tensor], space_transform_scale: Optional[float], space_transform_translate: Optional[th.Tensor]) -> tuple[dict[str, th.Tensor], float, th.Tensor]:\n",
    "        # If space transform is not given, initialize transform parameters from data\n",
    "        # NOTE: Assuming camera_to_world has scale 1\n",
    "        camera_positions = th.stack(tuple(camera_to_world.values()))[:, :3, -1] \n",
    "\n",
    "        # If no scale is given, initialize to 3*the maximum distance of any two cameras\n",
    "        if space_transform_scale is None:\n",
    "            space_transform_scale = 3*th.cdist(camera_positions, camera_positions, compute_mode=\"donot_use_mm_for_euclid_dist\").max().item()\n",
    "\n",
    "        # If no translation is given, initialize to mean\n",
    "        if space_transform_translate is None:\n",
    "            space_transform_translate = camera_positions.mean(dim=0)\n",
    "\n",
    "\n",
    "        # Only move the offset\n",
    "        translate_matrix = th.cat((space_transform_translate, th.zeros(1))).view(4, 1)\n",
    "        translate_matrix = th.hstack((th.zeros((4,3)), translate_matrix))\n",
    "\n",
    "        # Scale the camera distances\n",
    "        scale_matrix = th.ones((4, 4))\n",
    "        scale_matrix[:-1, -1] = space_transform_scale\n",
    "\n",
    "        # Move origin to average position of all cameras and scale world coordinates by the 3*the maximum distance of any two cameras\n",
    "        return (\n",
    "            { \n",
    "                image_name: (camera_to_world - translate_matrix)/scale_matrix\n",
    "                for image_name, camera_to_world \n",
    "                in camera_to_world.items() \n",
    "            },\n",
    "            space_transform_scale,\n",
    "            space_transform_translate\n",
    "        )\n",
    "\n",
    "    def _get_raw_rays(self, camera_to_world: dict[str, th.Tensor], image_width: int, image_height: int, focal_length: float) -> tuple[dict[str, th.Tensor], dict[str, th.Tensor]]:\n",
    "        # Create unit directions (H, W, 3) in camera space\n",
    "        # NOTE: Initially normalized such that z=-1 via the focal length.\n",
    "        #  Camera is looking in the negative z direction.\n",
    "        #  y-axis is also flipped.\n",
    "        y, x = th.meshgrid(\n",
    "            -th.linspace(-(image_height-1)/2, (image_height-1)/2, image_height) / focal_length,\n",
    "            th.linspace(-(image_width-1)/2, (image_width-1)/2, image_width) / focal_length,\n",
    "            indexing=\"ij\"\n",
    "        )\n",
    "        directions = th.stack((x, y, -th.ones_like(x)), dim=-1)\n",
    "        directions /= th.norm(directions, p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # Return rays keyed by image\n",
    "        return (\n",
    "            # Origins: Key by image and get focal points directly from camera to world projection\n",
    "            {\n",
    "                image_name: camera_to_world[:3, 3].expand_as(directions)\n",
    "                for image_name, camera_to_world in camera_to_world.items()\n",
    "            },\n",
    "            # Directions: Key by image and get directions directly from camera to world projection\n",
    "            # Rotate directions (H, W, 3) to world via R (3, 3).\n",
    "            #  Denote dir (row vector) as one of the directions in the directions tensor.\n",
    "            #  Then R @ dir.T = (dir @ R.T).T. \n",
    "            #  This would yield a column vector as output. \n",
    "            #  To get a row vector as output again, simply omit the last transpose.\n",
    "            #  The inside of the parenthesis on the right side \n",
    "            #  is conformant for matrix multiplication with the directions tensor.\n",
    "            # NOTE: Assuming scale of projection matrix is 1\n",
    "            { \n",
    "                image_name: directions @ camera_to_world[:3, :3].T\n",
    "                for image_name, camera_to_world in camera_to_world.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _get_noisy_rays(self, origins: dict[str, th.Tensor], directions: dict[str, th.Tensor], rotation_noise_sigma: Optional[float], translation_noise_sigma: Optional[float], noise_seed: Optional[int]) -> tuple[dict[str, th.Tensor], dict[str, th.Tensor]]:\n",
    "        # Get amount of cameras\n",
    "        n_cameras = len(origins)\n",
    "        \n",
    "        # Instantiate random number generator\n",
    "        rng = th.Generator()\n",
    "        if noise_seed is not None:\n",
    "            rng.manual_seed(noise_seed)\n",
    "\n",
    "\n",
    "        # Get random rotation amount in radians\n",
    "        thetas = th.randn((n_cameras, 1, 1), generator=rng) * rotation_noise_sigma\n",
    "\n",
    "        # Get random rotation axis\n",
    "        # NOTE: This is a random point on the unit sphere\n",
    "        # WARN: Technically a division by zero can happen.\n",
    "        #  This is however mitigated by applying the Ostrich algorithm :)\n",
    "        axes = th.randn((n_cameras, 3, 1), generator=rng)\n",
    "        axes /= th.norm(axes, p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        # Get rotation matrices via exponential map from lie algebra so(3) -> SO(3)\n",
    "        so3 = th.cross(\n",
    "            -th.eye(3).view(1, 3, 3), \n",
    "            thetas * axes,\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        rotations = th.matrix_exp(so3)\n",
    "\n",
    "\n",
    "        # Get random translation amount\n",
    "        translations = th.randn((n_cameras, 3), generator=rng) * translation_noise_sigma\n",
    "\n",
    "\n",
    "        # Return rays keyed by image\n",
    "        return (\n",
    "            # Origins: Key by image and move focal point\n",
    "            {\n",
    "                image_name: origins + trans\n",
    "                for (image_name, origins), trans in zip(origins.items(), translations)\n",
    "            },\n",
    "            # Directions: Key by image and get directions directly from camera to world projection\n",
    "            # Rotate directions (H, W, 3) via R (3, 3).\n",
    "            #  Denote dir (row vector) as one of the directions in the directions tensor.\n",
    "            #  Then R @ dir.T = (dir @ R.T).T. \n",
    "            #  This would yield a column vector as output. \n",
    "            #  To get a row vector as output again, simply omit the last transpose.\n",
    "            #  The inside of the parenthesis on the right side \n",
    "            #  is conformant for matrix multiplication with the directions tensor.\n",
    "            { \n",
    "                image_name: directions @ rot.T\n",
    "                for (image_name, directions), rot in zip(directions.items(), rotations)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _get_gaussian_blur_kernel(self, kernel_size: int, relative_sigma: float, max_side_length: int) -> th.Tensor:\n",
    "        # If sigma is 0, return a Dirac delta kernel\n",
    "        if relative_sigma <= sys.float_info.epsilon:\n",
    "            kernel = th.zeros(kernel_size)\n",
    "            kernel[kernel_size//2] = 1\n",
    "        # Else, create 1D Gaussian kernel\n",
    "        # NOTE: Gaussian blur is separable, so 1D kernel can simply be applied twice\n",
    "        else:\n",
    "            kernel = th.linspace(-kernel_size/2, kernel_size/2, kernel_size)\n",
    "            # Calculate inplace exp(-x^2 / (2 * (relative_sigma*max_side_length)^2))\n",
    "            kernel.square_().divide_(-2 * (relative_sigma * max_side_length)**2).exp_()\n",
    "            # Normalize the kernel\n",
    "            kernel.divide_(kernel.sum())\n",
    "\n",
    "\n",
    "        return kernel\n",
    "\n",
    "    def _get_blurred_pixel(self, img: th.Tensor, x: int, y: int, gaussian_blur_kernel: th.Tensor):\n",
    "        # NOTE: Assuming x and y are within bounds of img\n",
    "\n",
    "        # Retrive kernel dimensions\n",
    "        kernel_size = gaussian_blur_kernel.shape[0]\n",
    "        kernel_half = kernel_size//2\n",
    "\n",
    "        # Retrieve image dimensions\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Calculate padding\n",
    "        left = max(kernel_half - x, 0)\n",
    "        top = max(kernel_half - y, 0)\n",
    "        right = max(kernel_half + x - (img_width-1), 0)\n",
    "        bottom = max(kernel_half + y - (img_height-1), 0)\n",
    "\n",
    "        pad = tv.transforms.Pad(\n",
    "            padding=(left, top, right, bottom), \n",
    "            padding_mode=\"reflect\"\n",
    "        )\n",
    "\n",
    "        # Pad image and retrieve pixel and neighbors\n",
    "        neighborhood: th.Tensor = pad(img.permute(2, 0, 1))[\n",
    "            :,\n",
    "            (top+y-kernel_half):(top+y+kernel_half)+1, \n",
    "            (left+x-kernel_half):(left+x+kernel_half)+1,\n",
    "        ].permute(1, 2, 0)\n",
    "\n",
    "\n",
    "        # Blur y-direction and store y-column of pixel\n",
    "        # (H, W, C) -> (W, C)\n",
    "        blurred_y = (neighborhood * gaussian_blur_kernel.view(-1, 1, 1)).sum(dim=0)\n",
    "        # Blur x-direction and store pixel\n",
    "        # (W, C) -> (C)\n",
    "        blurred_pixel = (blurred_y * gaussian_blur_kernel.view(-1, 1)).sum(dim=0)\n",
    "\n",
    "        # Return blurred pixel\n",
    "        return blurred_pixel\n",
    "\n",
    "    def gaussian_blur_step(self) -> None:\n",
    "        # Update current variance\n",
    "        self.gaussian_blur_relative_sigma_current *= self.gaussian_blur_relative_sigma_decay\n",
    "        # Get new kernel\n",
    "        self.gaussian_blur_kernel = self._get_gaussian_blur_kernel(\n",
    "            self.gaussian_blur_kernel_size,\n",
    "            self.gaussian_blur_relative_sigma_current,\n",
    "            max(self.image_height, self.image_width)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int) -> DatasetOutput:\n",
    "        # Get image index\n",
    "        img_idx = index // self.image_batch_size\n",
    "\n",
    "        # Get dataset via image index\n",
    "        P, o_r, o_n, d_r, d_n, img = self.dataset[img_idx]\n",
    "        # Get pixel index\n",
    "        i = index % self.image_batch_size\n",
    "        \n",
    "        # Get raw pixel color\n",
    "        c_r = img.view(-1, 3)[i]\n",
    "\n",
    "        # If no blur, set color to current pixel\n",
    "        if self.gaussian_blur_relative_sigma_current <= sys.float_info.epsilon:\n",
    "            c_b = c_r\n",
    "        # Else, calculate color via gaussian blur\n",
    "        else:\n",
    "            c_b = self._get_blurred_pixel(\n",
    "                img, \n",
    "                i % self.image_width, \n",
    "                i // self.image_width, \n",
    "                self.gaussian_blur_kernel\n",
    "            )\n",
    "\n",
    "\n",
    "        return (\n",
    "            o_r.view(-1, 3)[i], \n",
    "            o_n.view(-1, 3)[i], \n",
    "            d_r.view(-1, 3)[i], \n",
    "            d_n.view(-1, 3)[i], \n",
    "            c_r,\n",
    "            c_b, \n",
    "            th.tensor(self.gaussian_blur_relative_sigma_current), \n",
    "            th.tensor(img_idx)\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset) * self.image_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate dataset \n",
    "BATCH_SIZE = 200 \n",
    "NUM_WORKERS = 1\n",
    "batch_size=BATCH_SIZE,\n",
    "num_workers=NUM_WORKERS,\n",
    "shuffle=True,\n",
    "pin_memory=True\n",
    "\n",
    "images_path = os.path.join(\"../data/lego\", \"train\").replace(\"\\\\\", \"/\")\n",
    "camera_info_path = os.path.join(\"../data/lego\", f\"transforms_train.json\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "dataset = ImagePoseDataset(\n",
    "    image_width=400,\n",
    "    image_height=400,\n",
    "    images_path=images_path,\n",
    "    camera_info_path=camera_info_path,\n",
    "    space_transform_scale=None,\n",
    "    space_transform_translate=None,\n",
    "    rotation_noise_sigma=0,\n",
    "    translation_noise_sigma=0,\n",
    "    noise_seed=54,\n",
    "    gaussian_blur_kernel_size=81,\n",
    "    gaussian_blur_relative_sigma_start=0.,\n",
    "    gaussian_blur_relative_sigma_decay=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "new_ds = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in new_ds:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
